{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# direktory\n",
    "train_path = './data/train/'\n",
    "test_path = './data/test/' \n",
    "\n",
    "weight, height = 32, 32\n",
    "batch_size = 32\n",
    "\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale = 1.0/255\n",
    ")\n",
    "test_data_gen = ImageDataGenerator(\n",
    "    rescale = 1.0/255\n",
    ")\n",
    "\n",
    "train_dataset = train_data_gen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size = (weight, height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'binary',\n",
    "    shuffle = True\n",
    ")\n",
    "test_dataset = test_data_gen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size = (weight, height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'binary',\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAKE': 0, 'REAL': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check kelas\n",
    "train_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224,833\n",
      "Trainable params: 224,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mendefinisikan model\n",
    "model = models.Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape=(weight, height, 3) ))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# dense layers\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summary model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "weight_path = '../code/'\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath = weight_path,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True,\n",
    "    mode = 'max',\n",
    "    monitor = 'val_accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3125/3125 [==============================] - 1682s 537ms/step - loss: 0.3366 - accuracy: 0.8547 - val_loss: 0.2454 - val_accuracy: 0.9008\n",
      "Epoch 2/2\n",
      "3125/3125 [==============================] - 423s 135ms/step - loss: 0.2319 - accuracy: 0.9070 - val_loss: 0.2192 - val_accuracy: 0.9128\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs = 2,\n",
    "                    validation_data = test_dataset,\n",
    "                    callbacks = [model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 40s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "# load best weigh to model\n",
    "model.load_weights(weight_path)\n",
    "\n",
    "# buat prediksi\n",
    "prediksi = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_predictions = (prediksi > 0.5).astype(int)\n",
    "binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# true label\n",
    "true_labels = test_dataset.classes\n",
    "\n",
    "# buat confussion matrix\n",
    "sns.heatmap(confusion_matrix(true_labels, binary_predictions), annot=True)\n",
    "\n",
    "# print report classification\n",
    "classification_report(true_labels, binary_predictions)\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Prediksi\")\n",
    "plt.ylabel(\"Actual Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariop\\AppData\\Local\\Temp\\ipykernel_17984\\3289309780.py:11: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  upscaled_image = image.resize((image.width * scale_factor, image.height * scale_factor), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "# test upscaling\n",
    "from PIL import Image\n",
    "\n",
    "# Baca citra dengan Pillow\n",
    "image = Image.open('./data/test/FAKE/1.jpg')\n",
    "\n",
    "# Tentukan faktor penskalaan (misalnya, 4 kali lipat)\n",
    "scale_factor = 10\n",
    "\n",
    "# Upscale citra menggunakan Pillow\n",
    "upscaled_image = image.resize((image.width * scale_factor, image.height * scale_factor), Image.ANTIALIAS)\n",
    "\n",
    "# Simpan citra hasil upscaling\n",
    "upscaled_image.save('./upscaled_image.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_zip_file(file_path, extract_path):\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "# Usage\n",
    "# Provide the path to the zip file and the path where it should be extracted\n",
    "extract_zip_file('./data/dataset.zip', './data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Open the zip file in read mode\n",
    "with zipfile.ZipFile('./data/dataset.zip', 'r') as zip_ref:\n",
    "    # Loop through each file\n",
    "    for file in zip_ref.namelist():\n",
    "        # Open each file\n",
    "        with zip_ref.open(file) as f:\n",
    "            # Read the file\n",
    "            print(f.read())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
